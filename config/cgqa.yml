model:
  model_name: TOMCAT
  prompt_template: ["a photo of x x"]
  ctx_init: ["a photo of "]
  clip_model: "ViT-L/14"
  clip_arch: {Your_Clip_Path}/clip-vit-large-patch14/model.pt

  adapter_dim: 64
  adapter_dropout: 0.1
  comp_cls_loss_weight: 1
  use_adapter: True

train:
  dataset: cgqa
  dataset_path: {Your_Dataset_Path}/cgqa
  optimizer: Adam
  scheduler: StepLR
  step_size: 5
  gamma: 0.5
  lr: 0.000003125
  attr_dropout: 0
  weight_decay: 0.00001
  context_length: 10
  train_batch_size: 8
  gradient_accumulation_steps: 2
  seed: 0
  epochs: 50
  epoch_start: 0
  val_metric: best_AUC
  save_final_model: True
  use_mixed_precision: True

test:
  eval_batch_size_wo_tta: 64
  eval_batch_size: 1
  # load_model:
  topk: 1
  text_encoder_batch_size: 1024

  #open world
  open_world: False
  threshold:
  threshold_trials: 50
  bias: 0.001
  text_first: True
  #
  num_workers: 0 #2
  current_epoch: -1

tta:
  shot_capacity: 3
  cpu_cache: True

  eps: 0.001
  wd: 0.0001
  alpha: 0.5
  beta: 10
  text_lr: 0.00000625 #5e-7
  image_lr: 0.00000625 #5e-7
  theta: 2.0
  align_loss_weight: 1.75

  use_tta: True
  use_img_cache: True
  use_align_loss: True

others:
  device: cuda:1 #Your_Device
  save_path:  #Your_Save_Path

  load_model: #Your_Save_Model_Path
  load_model_path:  #Your_Save_Path

  wandb_net: online
  use_wandb: True
