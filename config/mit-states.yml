model:
  model_name: TOMCAT
  prompt_template: ["a photo of x x"]
  ctx_init: ["a photo of "]
  clip_model: "ViT-L/14"
  clip_arch: {Your_Clip_Path}/clip-vit-large-patch14/model.pt

  adapter_dim: 64
  adapter_dropout: 0.1
  comp_cls_loss_weight: 1
  use_adapter: True

train:
  dataset: mit-states
  dataset_path: {Your_Dataset_Path}/mit-states
  optimizer: Adam
  scheduler: StepLR
  step_size: 5
  gamma: 0.5
  lr: 0.0001
  attr_dropout: 0.3
  weight_decay: 0.0001
  context_length: 10
  train_batch_size: 8
  gradient_accumulation_steps: 8
  seed: 0
  epochs: 50
  epoch_start: 0
  val_metric: best_AUC
  save_final_model: True
  use_mixed_precision: True

test:
  eval_batch_size_wo_tta: 1
  eval_batch_size: 1
  # load_model:
  topk: 1
  text_encoder_batch_size: 1024

  #open world
  open_world: False
  threshold:
  threshold_trials: 50
  bias: 0.001
  text_first: True
  #
  num_workers:  0
  current_epoch: -1

tta:
  shot_capacity: 3
  cpu_cache: True

  eps: 0.001
  wd: 0.0001
  alpha: 1.25
  beta: 10
  text_lr: 0.000001 #5e-7
  image_lr: 0.000001 #5e-7
  theta: 1.5
  align_loss_weight: 2.5

  use_tta: True
  use_img_cache: True
  use_align_loss: True

others:
  device: cuda:1 #Your_Device
  save_path: ./result/mit-states/20250305_1 #Your_Save_Path

  load_model: ./result/mit-states/20250305_1/test_best.pt
  load_model_path: ./result/mit-states/20250305_1

  wandb_net: online
  use_wandb: True